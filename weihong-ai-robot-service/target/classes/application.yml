server:
  port: 9000 # 项目启动端口

spring:
  ai:
    deepseek:
      api-key: sk-19e5dfaac7f941368f84ae58701f2b76 # 填写 DeepSeek Api Key, 改成你自己的
      base-url: https://api.deepseek.com # DeepSeek 的请求 URL, 可不填，默认值为 api.deepseek.com
      chat:
        options:
          model: deepseek-chat # 使用哪个模型 chat V3
#          model: deepseek-reasoner # 使用哪个模型 R1
          temperature: 0.8 # 温度值

    ollama:
      base-url: http://localhost:11434 # Ollama 服务的访问地址, 11434 端口是 Ollama 默认的启动端口
      chat:
        options: # 模型参数
          model: qwen3:1.7b # 指定 Ollama 使用的大模型名称，根据你实际安装的来，我运行的是 14.7
          temperature: 0.7 # 温度值
logging:
  level:
    org.springframework.ai.chat.client.advisor: debug